# This repo uses the `src/gateway` submodule (agent-gateway) as the LLM gateway.
# It exposes a streaming OpenAI-compatible `/v1/responses` endpoint.
#
# Quickstart (local):
# 1) Configure `src/gateway/.env` with `OPENAI_KEY=...`
# 2) `make run-gateway`
# 3) `python scripts/run_canonical.py --doc canonical.txt --exhibit-id canonical`

# --- Gateway server (src/gateway) ---
# Configure the gateway in `src/gateway/.env` (see `src/gateway/.env.example`), e.g.:
# OPENAI_KEY=sk-...
# GATEWAY_TIMEOUT_SECONDS=30

# --- Pipeline client (this repo) ---
GATEWAY_HOST=127.0.0.1
GATEWAY_PORT=8000
# Optional override if you aren't using host/port:
# GATEWAY_URL=http://127.0.0.1:8000/v1/responses

# Model selector follows the gateway convention: "<provider>:<upstream_model>"
MODEL=openai:gpt-5
REASONING_EFFORT=medium

# Client-side HTTP timeout when waiting for the gateway stream.
GATEWAY_TIMEOUT_SECONDS=180

# Memory persistence (schemas/champions keyed by goal_id)
EDGAR_AI_MEMORY_DIR=memory
